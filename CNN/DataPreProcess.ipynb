{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbe10ef-4716-4279-85bf-902777c7cb10",
   "metadata": {},
   "source": [
    "ANNOTATION CHANGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0add459f-2861-4e92-833e-907423a39e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def coco_to_vgg(coco_annotation):\n",
    "    vgg_annotation = {}\n",
    "\n",
    "    for image_info in coco_annotation['images']:\n",
    "        image_id = str(image_info['id'])\n",
    "        vgg_annotation[image_id] = {\n",
    "            \"filename\": image_info['file_name'],\n",
    "            \"size\": image_info['height'] * image_info['width'],\n",
    "            \"regions\": [],\n",
    "            \"file_attributes\": {\n",
    "                \"caption\": \"\",\n",
    "                \"public_domain\": \"no\",\n",
    "                \"image_url\": \"\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for annotation in coco_annotation['annotations']:\n",
    "            if annotation['image_id'] == int(image_id):\n",
    "                region = {\n",
    "                    \"shape_attributes\": {\n",
    "                        \"name\": \"polygon\",\n",
    "                        \"all_points_x\": annotation['segmentation'][0][0::2],\n",
    "                        \"all_points_y\": annotation['segmentation'][0][1::2],\n",
    "                    },\n",
    "                    \"region_attributes\": {\n",
    "                        \"image_quality\": {\n",
    "                            \"good\": True,\n",
    "                            \"frontal\": True,\n",
    "                            \"good_illumination\": True\n",
    "                        },\n",
    "                        \"names\": coco_annotation['categories'][annotation['category_id']]['name']\n",
    "                    }\n",
    "                }\n",
    "                vgg_annotation[image_id]['regions'].append(region)\n",
    "\n",
    "    return vgg_annotation\n",
    "\n",
    "def convert_and_save(input_path, output_path):\n",
    "    with open(input_path, 'r') as file:\n",
    "        coco_annotation = json.load(file)\n",
    "\n",
    "    vgg_annotation = coco_to_vgg(coco_annotation)\n",
    "\n",
    "    with open(output_path, 'w') as file:\n",
    "        json.dump(vgg_annotation, file, indent=2)\n",
    "\n",
    "# Input and output file paths\n",
    "input_file_path = 'C:\\\\Users\\\\afiqe\\\\Downloads\\\\FYP DATASET LAMA.v10i.coco-segmentation\\\\dataset\\\\test\\\\_annotations.coco.json'\n",
    "output_file_path = 'C:\\\\Users\\\\afiqe\\\\Downloads\\\\FYP DATASET LAMA.v10i.coco-segmentation\\\\dataset\\\\test\\\\_annotations1.coco.json'\n",
    "\n",
    "# Convert COCO to VGG and save to the output file\n",
    "convert_and_save(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e1a146e-db93-487b-8e97-4b7ef0b73493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 38\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def count_images(annotation_data):\n",
    "    return len(annotation_data)\n",
    "\n",
    "def main():\n",
    "    # Replace 'path_to_your_annotation_file.json' with the actual path to your annotation file\n",
    "    annotation_file_path = 'C:\\\\Users\\\\afiqe\\\\Downloads\\\\FYP DATASET LAMA.v10i.coco-segmentation\\\\dataset\\\\test\\\\_annotations1.coco.json'\n",
    "\n",
    "    try:\n",
    "        with open(annotation_file_path, 'r') as f:\n",
    "            annotation_data = json.load(f)\n",
    "\n",
    "            # Call the function to count images\n",
    "            num_images = count_images(annotation_data)\n",
    "\n",
    "            print(f\"Number of images: {num_images}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{annotation_file_path}' not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Unable to decode JSON from '{annotation_file_path}'. Please check if the file is valid JSON.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8380f-c522-442f-94c6-33b8ecb0196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def load_coco_annotation(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        coco_annotation = json.load(file)\n",
    "    return coco_annotation\n",
    "\n",
    "def visualize_masks(image, masks, class_ids, class_names):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    unique_class_ids = list(set(class_ids))\n",
    "    for i, class_id in enumerate(unique_class_ids):\n",
    "        mask = masks[:, :, class_ids == class_id]\n",
    "        color = plt.cm.get_cmap('tab20')(i / len(unique_class_ids))\n",
    "\n",
    "        for j in range(mask.shape[2]):\n",
    "            contour, _ = cv2.findContours(mask[:, :, j].astype('uint8'), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            for c in contour:\n",
    "                polygon = Polygon(c.reshape((-1, 2)), fill=False, edgecolor=color, linewidth=2)\n",
    "                ax.add_patch(polygon)\n",
    "\n",
    "    ax.set_title('Mask Visualization')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "coco_annotation_path = 'path/to/your/annotation.json'\n",
    "coco_annotation = load_coco_annotation(coco_annotation_path)\n",
    "\n",
    "# Replace image_id with the desired image index you want to visualize\n",
    "image_id = 0\n",
    "image_info = coco_annotation['images'][image_id]\n",
    "image = cv2.imread(image_info['file_name'])\n",
    "masks, class_ids = your_dataset.load_mask(image_id)  # Replace with your actual load_mask function\n",
    "\n",
    "# Replace your_dataset.class_names with the actual class names\n",
    "visualize_masks(image, masks, class_ids, your_dataset.class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c9668-a86b-48ed-ad12-9a41b6839c95",
   "metadata": {},
   "source": [
    "RESIZE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea1ff92-88c0-4178-a946-dd3e54ca398f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afiqe\\AppData\\Local\\Temp\\ipykernel_14580\\2306709436.py:28: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  image = image.resize(target_size, Image.ANTIALIAS)\n",
      "E:\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "input_folder = 'C:\\\\Users\\\\afiqe\\\\Downloads\\\\Dataset10Class'\n",
    "output_folder = 'C:\\\\Users\\\\afiqe\\\\Downloads\\\\DatasetPreProc'\n",
    "target_size = (200, 200)  # Set your desired size\n",
    "\n",
    "for class_name in os.listdir(input_folder):\n",
    "    class_input_folder = os.path.join(input_folder, class_name)\n",
    "    class_output_folder = os.path.join(output_folder, class_name)\n",
    "\n",
    "    if os.path.isdir(class_input_folder):\n",
    "        os.makedirs(class_output_folder, exist_ok=True)\n",
    "\n",
    "        for filename in os.listdir(class_input_folder):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n",
    "                image_path = os.path.join(class_input_folder, filename)\n",
    "                image = Image.open(image_path)\n",
    "\n",
    "                # Convert to 'RGB' mode if the image has a palette-based mode ('P')\n",
    "                if image.mode == 'P':\n",
    "                    image = image.convert('RGB')\n",
    "                    \n",
    "                if image.mode == 'RGBA':\n",
    "                    image = image.convert('RGB')\n",
    "\n",
    "                # Resize the image\n",
    "                image = image.resize(target_size, Image.ANTIALIAS)\n",
    "\n",
    "                # Save the resized image\n",
    "                output_path = os.path.join(class_output_folder, filename)\n",
    "                image.save(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a60bc-3e95-44c5-8ac1-8219bd9a5111",
   "metadata": {},
   "source": [
    "SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd1a575-7205-4408-badc-2168a3d3fa58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into training, validation, and test sets.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define the source directory containing your dataset\n",
    "source_dir = 'C:\\\\Users\\\\afiqe\\\\Downloads\\\\FYP\\\\Dataset\\\\Dataset10Class'\n",
    "\n",
    "# Define the target directories for training, validation, and testing\n",
    "train_dir = 'C:\\\\Users\\\\afiqe\\\\Downloads\\\\FYP\\\\Dataset\\\\Dataset60\\\\train'\n",
    "val_dir = 'C:\\\\Users\\\\afiqe\\\\Downloads\\\\FYP\\\\Dataset\\\\Dataset60\\\\val'\n",
    "test_dir = 'C:\\\\Users\\\\afiqe\\\\Downloads\\\\FYP\\\\Dataset\\\\Dataset60\\\\test'\n",
    "\n",
    "# Create target directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Set the ratio for splitting the data\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.35\n",
    "test_ratio = 0.05\n",
    "\n",
    "# List the classes (subdirectories) in the source directory\n",
    "class_names = os.listdir(source_dir)\n",
    "\n",
    "# Loop through each class and split the data\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(source_dir, class_name)\n",
    "    file_list = os.listdir(class_dir)\n",
    "    \n",
    "    random.shuffle(file_list)  # Shuffle the files\n",
    "\n",
    "    num_files = len(file_list)\n",
    "    num_train = int(train_ratio * num_files)\n",
    "    num_val = int(val_ratio * num_files)\n",
    "    \n",
    "    train_files = file_list[:num_train]\n",
    "    val_files = file_list[num_train:num_train + num_val]\n",
    "    test_files = file_list[num_train + num_val:]\n",
    "\n",
    "    # Copy files to the target directories\n",
    "    for file in train_files:\n",
    "        src = os.path.join(class_dir, file)\n",
    "        dest = os.path.join(train_dir, class_name, file)\n",
    "        os.makedirs(os.path.dirname(dest), exist_ok=True)\n",
    "        shutil.copy(src, dest)\n",
    "\n",
    "    for file in val_files:\n",
    "        src = os.path.join(class_dir, file)\n",
    "        dest = os.path.join(val_dir, class_name, file)\n",
    "        os.makedirs(os.path.dirname(dest), exist_ok=True)\n",
    "        shutil.copy(src, dest)\n",
    "\n",
    "    for file in test_files:\n",
    "        src = os.path.join(class_dir, file)\n",
    "        dest = os.path.join(test_dir, class_name, file)\n",
    "        os.makedirs(os.path.dirname(dest), exist_ok=True)\n",
    "        shutil.copy(src, dest)\n",
    "\n",
    "print(\"Dataset split into training, validation, and test sets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709744c-597d-46a2-ac7c-48015486c12d",
   "metadata": {},
   "source": [
    "DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ad69b0-2171-4d6f-9664-0f3151e384bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation completed.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the source directory containing your dataset\n",
    "source_dir = 'C:\\\\Users\\\\afiqe\\\\Downloads\\\\DatasetSplit\\\\test'\n",
    "\n",
    "# Define the target directory for augmented data\n",
    "output_dir = 'C:\\\\Users\\\\afiqe\\\\Downloads\\\\DatasetNew\\\\test'\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List the classes (subdirectories) in the source directory\n",
    "class_names = os.listdir(source_dir)\n",
    "\n",
    "# Define the augmentation factors\n",
    "zoom_factor = 1.2  # Adjust the factor as needed\n",
    "rotation_angles = [-10, 10]\n",
    "blur_kernel_sizes = [3, 5]\n",
    "sharpen_kernel = np.array([[-1, -1, -1],\n",
    "                           [-1, 9, -1],\n",
    "                           [-1, -1, -1]], dtype=np.float32)\n",
    "\n",
    "# Function to apply and save data augmentation to an image\n",
    "def apply_augmentation(image, class_name, output_dir, filename):\n",
    "    if image is not None:\n",
    "        # Save the original image\n",
    "        original_save_path = os.path.join(output_dir, class_name, f'{filename}_original.jpg')\n",
    "        cv2.imwrite(original_save_path, image)\n",
    "        \n",
    "        # Apply zooming\n",
    "        h, w, _ = image.shape\n",
    "        new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)\n",
    "        zoomed_image = cv2.resize(image, (new_w, new_h))\n",
    "        h_diff, w_diff = (new_h - h) // 2, (new_w - w) // 2\n",
    "        cropped_image = zoomed_image[h_diff:h_diff+h, w_diff:w_diff+w]\n",
    "        save_path = os.path.join(output_dir, class_name, f'{filename}_{int(zoom_factor * 100)}_zoom.jpg')\n",
    "        cv2.imwrite(save_path, cropped_image)\n",
    "        \n",
    "        for angle in rotation_angles:\n",
    "            rotated_image = cv2.warpAffine(image, cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1), (w, h))\n",
    "            save_path = os.path.join(output_dir, class_name, f'{filename}_{angle}_rotate.jpg')\n",
    "            cv2.imwrite(save_path, rotated_image)\n",
    "        \n",
    "        for kernel_size in blur_kernel_sizes:\n",
    "            blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n",
    "            save_path = os.path.join(output_dir, class_name, f'{filename}_{kernel_size}_blur.jpg')\n",
    "            cv2.imwrite(save_path, blurred_image)\n",
    "        \n",
    "        sharpened_image = cv2.filter2D(image, -1, sharpen_kernel)\n",
    "        save_path = os.path.join(output_dir, class_name, f'{filename}_sharpen.jpg')\n",
    "        cv2.imwrite(save_path, sharpened_image)\n",
    "\n",
    "        # Horizontal flip\n",
    "        flipped_horizontal_image = cv2.flip(image, 1)\n",
    "        save_path = os.path.join(output_dir, class_name, f'{filename}_flip_horizontal.jpg')\n",
    "        cv2.imwrite(save_path, flipped_horizontal_image)\n",
    "\n",
    "        # Vertical flip\n",
    "        flipped_vertical_image = cv2.flip(image, 0)\n",
    "        save_path = os.path.join(output_dir, class_name, f'{filename}_flip_vertical.jpg')\n",
    "        cv2.imwrite(save_path, flipped_vertical_image)\n",
    "\n",
    "# Loop through each class and apply data augmentation\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(source_dir, class_name)\n",
    "    \n",
    "    # Create subdirectories for each class in the output directory\n",
    "    os.makedirs(os.path.join(output_dir, class_name), exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(class_dir):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp')):  # Add more extensions if needed\n",
    "            image_path = os.path.join(class_dir, filename)\n",
    "            \n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            # Apply data augmentation and save augmented images\n",
    "            apply_augmentation(image, class_name, output_dir, filename)\n",
    "\n",
    "print(\"Data augmentation completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7a3b2-5e91-453e-854a-9d306b972c38",
   "metadata": {},
   "source": [
    "DATA AUGMENTATION YOLOV8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ed17b-4259-41e1-9967-d23f1d882469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def flip_horizontal(image, boxes):\n",
    "    return cv2.flip(image, 1), boxes\n",
    "\n",
    "def flip_vertical(image, boxes):\n",
    "    return cv2.flip(image, 0), boxes\n",
    "\n",
    "def rotate(image, boxes, angle=30):\n",
    "    rows, cols, _ = image.shape\n",
    "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "    image = cv2.warpAffine(image, M, (cols, rows))\n",
    "    \n",
    "    # Update bounding box coordinates\n",
    "    new_boxes = []\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box\n",
    "        x, y = np.dot(M, [x, y, 1])[:-1]\n",
    "        new_boxes.append([x, y, w, h])\n",
    "    \n",
    "    return image, new_boxes\n",
    "\n",
    "def blur(image, boxes, ksize=(5, 5)):\n",
    "    return cv2.GaussianBlur(image, ksize, 0), boxes\n",
    "\n",
    "def crop(image, boxes, percent=0.2):\n",
    "    rows, cols, _ = image.shape\n",
    "    y1 = int(rows * percent)\n",
    "    y2 = int(rows * (1 - percent))\n",
    "    x1 = int(cols * percent)\n",
    "    x2 = int(cols * (1 - percent))\n",
    "    \n",
    "    image = image[y1:y2, x1:x2]\n",
    "    \n",
    "    # Update bounding box coordinates\n",
    "    new_boxes = []\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box\n",
    "        x, y = x - x1, y - y1\n",
    "        new_boxes.append([x, y, w, h])\n",
    "    \n",
    "    return image, new_boxes\n",
    "\n",
    "def read_annotations(annotation_path):\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    boxes = [list(map(float, line.strip().split()[1:])) for line in lines]\n",
    "    return np.array(boxes)\n",
    "\n",
    "def write_annotations(annotation_path, boxes):\n",
    "    with open(annotation_path, 'w') as f:\n",
    "        for box in boxes:\n",
    "            f.write(\"0 \" + \" \".join(map(str, box)) + '\\n')\n",
    "\n",
    "def augment_data(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(input_folder, file_name)\n",
    "            annotation_path = os.path.join(input_folder, os.path.splitext(file_name)[0] + '.txt')\n",
    "            \n",
    "            image = cv2.imread(image_path)\n",
    "            boxes = read_annotations(annotation_path)\n",
    "            \n",
    "            # Augmentation functions\n",
    "            augmentations = [flip_horizontal, flip_vertical, rotate, blur, crop]\n",
    "            \n",
    "            for aug_func in augmentations:\n",
    "                augmented_image, augmented_boxes = aug_func(image.copy(), boxes.copy())\n",
    "                \n",
    "                output_image_path = os.path.join(output_folder, f\"{aug_func.__name__}_{file_name}\")\n",
    "                output_annotation_path = os.path.join(output_folder, f\"{aug_func.__name__}_{os.path.splitext(file_name)[0]}.txt\")\n",
    "                \n",
    "                cv2.imwrite(output_image_path, augmented_image)\n",
    "                write_annotations(output_annotation_path, augmented_boxes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"path/to/your/dataset\"\n",
    "    output_folder = \"path/to/your/augmented/dataset\"\n",
    "    \n",
    "    augment_data(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7092cd0-8c85-45f0-849d-697a7950ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def draw_boxes(image, boxes, color=(0, 255, 0), thickness=2):\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box\n",
    "        x1, y1 = int(x - w / 2), int(y - h / 2)\n",
    "        x2, y2 = int(x + w / 2), int(y + h / 2)\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "def visualize_augmented_images(input_folder, output_folder, num_images=5):\n",
    "    for file_name in os.listdir(input_folder)[:num_images]:\n",
    "        if file_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(input_folder, file_name)\n",
    "            annotation_path = os.path.join(input_folder, os.path.splitext(file_name)[0] + '.txt')\n",
    "            \n",
    "            image = cv2.imread(image_path)\n",
    "            original_boxes = read_annotations(annotation_path)\n",
    "            \n",
    "            # Draw bounding boxes on the original image\n",
    "            original_image = image.copy()\n",
    "            draw_boxes(original_image, original_boxes)\n",
    "            \n",
    "            cv2.imshow(\"Original Image\", original_image)\n",
    "            cv2.waitKey(0)\n",
    "            \n",
    "            # Show augmented images with bounding boxes\n",
    "            for aug_func in augmentations:\n",
    "                augmented_image_path = os.path.join(output_folder, f\"{aug_func.__name__}_{file_name}\")\n",
    "                augmented_annotation_path = os.path.join(output_folder, f\"{aug_func.__name__}_{os.path.splitext(file_name)[0]}.txt\")\n",
    "                \n",
    "                augmented_image = cv2.imread(augmented_image_path)\n",
    "                augmented_boxes = read_annotations(augmented_annotation_path)\n",
    "                \n",
    "                # Draw bounding boxes on the augmented image\n",
    "                augmented_image_with_boxes = augmented_image.copy()\n",
    "                draw_boxes(augmented_image_with_boxes, augmented_boxes)\n",
    "                \n",
    "                cv2.imshow(f\"{aug_func.__name__} Augmentation\", augmented_image_with_boxes)\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"path/to/your/dataset\"\n",
    "    output_folder = \"path/to/your/augmented/dataset\"\n",
    "    \n",
    "    visualize_augmented_images(input_folder, output_folder)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530a957-ae31-4f19-8c85-991c25f7132f",
   "metadata": {},
   "source": [
    "FIND UNICODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cbfa267-5527-4262-a355-1d6bb8ae7737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import codecs\n",
    "\n",
    "def fix_non_utf8_characters(src_directory, dest_directory):\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    os.makedirs(dest_directory, exist_ok=True)\n",
    "\n",
    "    # List files in the source directory\n",
    "    files = os.listdir(src_directory)\n",
    "\n",
    "    for file in files:\n",
    "        src_path = os.path.join(src_directory, file)\n",
    "        \n",
    "        if os.path.isfile(src_path):\n",
    "            # Handle non-UTF-8 characters in the file name\n",
    "            try:\n",
    "                # Decode the filename using 'utf-8'\n",
    "                decoded_filename = file.encode('latin-1').decode('utf-8')\n",
    "                dest_path = os.path.join(dest_directory, decoded_filename)\n",
    "\n",
    "                # Copy the file to the destination directory with the decoded filename\n",
    "                shutil.copy(src_path, dest_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {file} - {e}\")\n",
    "                continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    src_directory = \"C:\\\\Users\\\\afiqe\\\\Downloads\\\\DatasetNew\\\\Dataset\\\\nasi lemak\"\n",
    "    dest_directory = \"C:\\\\Users\\\\afiqe\\\\Downloads\\\\DatasetNew\\\\nasi lemak\"\n",
    "\n",
    "    # Call the function to copy files with non-UTF-8 characters\n",
    "    fix_non_utf8_characters(src_directory, dest_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
